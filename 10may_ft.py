# -*- coding: utf-8 -*-
"""10MAY_FT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MsNVF_xcJBG4c74FVtpZhSGlI7foeztB
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install torch
!pip install datasets
!pip install sentencepiece
!pip install transformers
from datasets import load_dataset, Dataset, DatasetDict

from datasets import load_dataset

# Define the unwanted labels you want to map to "O"
unwanted_labels = ["B-LOC", "I-LOC", "B-MISC", "I-MISC"]

# Define your label map
label_map = {
    "O": 0,
    "B-PER": 1,
    "I-PER": 2,
    "B-ORG": 3,
    "I-ORG": 4,
    "B-COMM": 5,
    "I-COMM": 6
}

# Load your dataset (replace with your actual dataset path)
dataset_path = "/content/drive/MyDrive/Model_V3/Dataset/Train_NER_10MAY.txt"
dataset = load_dataset("text", data_files=dataset_path)

# Function to parse text and extract tokens with labels
def parse_text(example):
    words, labels = [], []
    text_data = example.get("text", "").strip()

    if not text_data:
        return {"tokens": [], "ner_tags": []}

    # Assuming your data is formatted like "word1 O word2 B-PER ..."
    tokens = text_data.split(" ")
    i = 0
    while i < len(tokens):
        if i + 1 < len(tokens) and tokens[i + 1] in label_map:
            words.append(tokens[i])
            labels.append(label_map.get(tokens[i + 1], 0))  # Default to 'O'
            i += 2
        else:
            words.append(tokens[i])
            labels.append(label_map.get("O", 0))  # Default to 'O'
            i += 1

    return {"tokens": words, "ner_tags": labels}

# Apply parsing function
dataset = dataset.map(parse_text)

# Define the function to map unwanted labels to "O"
def map_unwanted_labels(example):
    # Update labels to "O" (0) if they are in unwanted_labels
    new_labels = []
    for label in example["ner_tags"]:
        # Get the label text from label_map and map unwanted labels to "O"
        label_text = list(label_map.keys())[list(label_map.values()).index(label)] if label != -100 else "O"
        if label_text in unwanted_labels:
            new_labels.append(label_map["O"])  # Map to O (0)
        else:
            new_labels.append(label)
    example["ner_tags"] = new_labels
    return example

# Apply the function to update the labels
dataset = dataset.map(map_unwanted_labels)

from datasets import load_dataset
from transformers import AutoModelForTokenClassification, AutoTokenizer, DataCollatorForTokenClassification
import os

# Define the label mapping from the provided model

label_map = {
     "O": 0,  # Outside of a named entity
    "B-PER": 1,
    "I-PER": 2,
    "B-ORG": 3,
    "I-ORG": 4,
    "B-COMM": 5,
    "I-COMM": 6
}

id2label = {v: k for k, v in label_map.items() if v != -100}
label2id = {k: v for v, k in id2label.items()}
num_labels = len(set(label_map.values()))  # Count unique label indices
# Load pre-trained model with correct label mapping

# Load pre-trained model and tokenizer
model_name = "Lokeshwaran/xlm-roberta-base-fintuned-panx-ta-hi"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(
    model_name,
    num_labels=num_labels,
    id2label=id2label,
    label2id=label2id,
    ignore_mismatched_sizes=True
)

# Load dataset
dataset_path = "/content/drive/MyDrive/Model_V3/Dataset/Train_NER_10MAY.txt"
dataset = load_dataset("text", data_files=dataset_path)

# Function to parse text and extract tokens with labels
def parse_text(example):
    words, labels = [], []
    text_data = example.get("text", "").strip()

    if not text_data:
        return {"tokens": [], "ner_tags": []}

    tokens = text_data.split(" ")
    i = 0
    while i < len(tokens):
        if i + 1 < len(tokens) and tokens[i + 1] in label_map:
            words.append(tokens[i])
            labels.append(label_map.get(tokens[i + 1], 0))  # Default to 'O'
            i += 2
        else:
            words.append(tokens[i])
            labels.append(label_map.get("O", 0))
            i += 1

    return {"tokens": words, "ner_tags": labels}

# Apply parsing function
dataset = dataset.map(parse_text)

def tokenize_and_align_labels(batch):
    tokenized_inputs = tokenizer(
        batch["tokens"],
        truncation=True,
        padding="max_length",
        max_length=128,
        is_split_into_words=True
    )

    all_labels = []

    for i in range(len(batch["tokens"])):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        labels = []

        for word_idx in word_ids:
            if word_idx is None:
                labels.append(0)  # Special tokens should be ignored
            else:
                label = batch["ner_tags"][i][word_idx]
                labels.append(label)  # Assign 0 for unused labels

        labels += [0] * (len(tokenized_inputs["input_ids"][i]) - len(labels))
        all_labels.append(labels)

    tokenized_inputs["labels"] = all_labels
    return tokenized_inputs
tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)

print(model.config.id2label)
print(model.config.label2id)

from transformers import TrainingArguments

checkpoint_path = "/content/drive/MyDrive/Model_V3/Checkpoints/checkpoint-5288"  # Replace with your actual checkpoint path

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/Model_V3/12May2025",  # Your main output directory
    overwrite_output_dir=True,  # Overwrite if needed
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=5,
    save_strategy="epoch",
    save_total_limit=2,
    learning_rate=2e-5,
    weight_decay=0.01,
    logging_dir="/content/drive/MyDrive/Model_V3/12May2025/Logs",
    logging_steps=10,
    report_to="wandb",  # If using Weights & Biases
    resume_from_checkpoint=checkpoint_path  # Resume from the checkpoint

    evaluation_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",  # or "accuracy" if you're logging that
    greater_is_better=False
)

from transformers import Trainer
data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    tokenizer=tokenizer,
    data_collator=data_collator
)

# Resume training
trainer.train(resume_from_checkpoint=checkpoint_path)

from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline

text = "கமல் கிரிக்கெட் விளையாடுகிறார்"

ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="first")
# Get predictions
ner_results = ner_pipeline(text)

# Display results
for entity in ner_results:
    print(f"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}")

import torch
torch.save(model, '/content/drive/MyDrive/Model_V3/model-v3.pth')

# Save to Google Drive in safetensors format
save_path = "/content/drive/MyDrive/Model_V3/model-v3"
model.save_pretrained(save_path, safe_serialization=True)
tokenizer.save_pretrained(save_path)

